{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time, shutil, re, csv, datetime, pickle, math \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.webdriver import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(\"--log-level=3\")  # fatal\n",
    "chrome_options.add_argument(\"--start-maximized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_path = 'CSVs/companies/companies.csv'\n",
    "companies_about_path = 'CSVs/linkedin_about/companies_about.csv'\n",
    "jobs_path = 'CSVs/linkedin_jobs/jobs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_list():\n",
    "    try: return pd.read_csv(clients_info_path)\n",
    "    except: return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_about_list():\n",
    "    try: return pd.read_csv(companies_about_path)\n",
    "    except: return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_list():\n",
    "    try: return pd.read_csv(jobs_path)\n",
    "    except: return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies_to_find():\n",
    "    df = get_companies_list()\n",
    "    try: return df['company'].to_list()\n",
    "    except: return []\n",
    "    \n",
    "get_companies_to_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_jobs_companies_list():\n",
    "    df = get_jobs_list()\n",
    "    try: return df['company'].to_list()\n",
    "    except: return []\n",
    "    \n",
    "get_jobs_companies_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linkedin_active_account():\n",
    "    username = 'zeipf5@gmail.com'\n",
    "    password = 'dextra#2021'\n",
    "    return username, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_linkedin():\n",
    "    \n",
    "    linkedin_login_url = 'https://www.linkedin.com/uas/login'\n",
    "    \n",
    "    try:\n",
    "        print('Accessing: ', linkedin_login_url)\n",
    "        browser.get(linkedin_login_url)\n",
    "        time.sleep(2 + random())\n",
    "    \n",
    "        username, password = get_linkedin_active_account()\n",
    "    \n",
    "        print('Sign up with: ', username)\n",
    "        username_field = browser.find_element_by_id('username')\n",
    "        username_field.send_keys(username)\n",
    "\n",
    "        password_field = browser.find_element_by_id('password')\n",
    "        password_field.send_keys(password)\n",
    "    \n",
    "        time.sleep(1 + random())\n",
    "        password_field.submit()\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_id(company):\n",
    "    \n",
    "    linkedin_search_url = 'https://www.linkedin.com/search/results/companies/?keywords=' + company\n",
    "    print('Accessing: ', linkedin_search_url)\n",
    "    browser.get(linkedin_search_url)\n",
    "    time.sleep(1 + random())\n",
    "    \n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    code = soup.find_all('code')\n",
    "\n",
    "    try:\n",
    "        for code_str in code:\n",
    "            \n",
    "            if('urn:li:fsd_company:' in code_str.text):\n",
    "                \n",
    "                company_id = re.findall(r\"(?<=fsd_company:)\\d+?(?=[,|\\\"])\", code_str.text)[0]\n",
    "                \n",
    "                if company_id == '' or company_id == '8175': \n",
    "                    return ''\n",
    "                \n",
    "        return company_id\n",
    "    \n",
    "    except:\n",
    "        print('ERROR: - Company ID except for: ', company)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_list(browser, retry: bool = True):    \n",
    "    try:\n",
    "        element = WebDriverWait(browser, 3).until(EC.presence_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages--number')))\n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        time.sleep(random())\n",
    "        li_list = soup.find_all('li',attrs={'data-occludable-entity-urn' :True}, recursive=True)\n",
    "        return li_list\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_description_on(browser, False)\n",
    "        else: return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_description_on(browser, retry: bool = True):\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    try:\n",
    "        job_description = soup.find(attrs={\"class\": \"jobs-description-content__text\"}).getText(separator=' ')\n",
    "        job_description = re.sub(r'\\ +', ' ', job_description.replace('\\n', '')).strip()\n",
    "        return job_description\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_description_on(browser, False)\n",
    "        else: return 'NF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_applicants(browser, retry: bool = True):\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    try:\n",
    "        applicants = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('\\d\\ applicant')).replace('\\n', '').strip()\n",
    "        applicants = re.sub(r'[a-zA-Z\\ ]', '', applicants)\n",
    "        return applicants\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_applicants(browser, False)\n",
    "        else: return 'NF'\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_employees(browser, retry: bool = True):\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    try:\n",
    "        employees = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('[\\+\\d]\\ employees')).replace('\\n', '').strip()\n",
    "        employees = re.sub(r'[a-zA-Z\\ ]', '', employees)\n",
    "        return employees\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_employees(browser, False)\n",
    "        else: return 'NF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_views(browser, retry: bool = True):\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    try:\n",
    "        views = soup.find(attrs={'class': 'jobs-details-top-card__content-container'}).find(text=re.compile('\\d\\ view')).replace('\\n', '').strip()\n",
    "        views = re.sub(r'[a-zA-Z\\ ]', '', views)\n",
    "        return views\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_views(browser, False)\n",
    "        else: return 'NF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_date(browser, retry: bool = True):\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    try:\n",
    "        job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "        job_date = job_li.find(\"time\")[\"datetime\"]\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "        if retry: return get_job_date(browser, False)\n",
    "        else: return 'NF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs(company, competitors, limit):\n",
    "    \n",
    "    #opening the csv to save the informations about the client\n",
    "    df_info = get_companies_list()\n",
    "    \n",
    "    #searching for the company's linkedin page\n",
    "    linkedin_search_url = 'https://www.linkedin.com/search/results/companies/?keywords=' + company\n",
    "    print(\"Acessing: \" + linkedin_search_url)\n",
    "    browser.get(linkedin_search_url)\n",
    "    time.sleep(1 + random())\n",
    "    \n",
    "    #getting company id\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    code = soup.find_all('code')\n",
    "    \n",
    "#    company_id = get_company_id(company)    \n",
    "#     if company_id == '':\n",
    "#         print('**Company id empty')\n",
    "#         return\n",
    "#     else:\n",
    "#        print('**Founded some company id')\n",
    "\n",
    "    company_id = ''\n",
    "    try:\n",
    "        for code_str in code:\n",
    "            if('urn:li:fsd_company:' in code_str.text):\n",
    "                company_id = re.findall(r\"(?<=fsd_company:)\\d+?(?=[,|\\\"])\", code_str.text)[0]\n",
    "                print(company_id)\n",
    "                if(company_id != '8175'):\n",
    "                    break\n",
    "        if company_id == '':\n",
    "            raise Exception(\"No company Id\")\n",
    "    except:\n",
    "        print(\"company \" + company + \" not found\\n\")\n",
    "        df_info = df_info.append({\n",
    "            'client': company, \n",
    "            'found': False, \n",
    "            'client id': '',\n",
    "            'competitors': competitors,\n",
    "            'employees': '',\n",
    "            'total jobs': '', \n",
    "            'collected jobs': ''\n",
    "        }, ignore_index=True)\n",
    "        df_info.to_csv(\"client_info.csv\", index=False)\n",
    "        return \n",
    "        \n",
    "    #accessing the company's job offers\n",
    "    linkedin_jobs_url = 'https://www.linkedin.com/jobs/search/?geoId=92000000&f_C=' + company_id\n",
    "    browser.get(linkedin_jobs_url)\n",
    "    \n",
    "    #getting the total pages of the job offers\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    \n",
    "    try:\n",
    "        total_jobs = re.search(r\"(.+\\ )\", soup.find(\"div\", attrs={\"class\": \"jobs-search-results-list__title-heading\"}).find(\"small\").text).group(1).strip()\n",
    "    except:\n",
    "        print(\"no jobs found for \" + company + '\\n')\n",
    "        df_info = df_info.append({\n",
    "            'client': company, \n",
    "            'found': True,\n",
    "            'client id': str(company_id),\n",
    "            'competitors': competitors, \n",
    "            'employees': '',\n",
    "            'total jobs': '0', \n",
    "            'collected jobs': '0'\n",
    "        }, ignore_index=True)\n",
    "        df_info.to_csv(\"client_info.csv\", index=False)\n",
    "        return\n",
    "    \n",
    "    total_jobs = int(re.sub(r'[\\.|,]', '', total_jobs))\n",
    "    \n",
    "    pages = math.ceil(int(total_jobs) / 25)\n",
    "    print(str(total_jobs) + ' jobs and ' + str(pages) + ' pages found for ' + company + \"\\n\")\n",
    "\n",
    "    #getting the information from all the jobs\n",
    "    jobs_computed = 0\n",
    "    jobs = []\n",
    "    \n",
    "    for page in range(pages):\n",
    "        \n",
    "        #access a job page\n",
    "        linkedin_jobs_page_url = 'https://www.linkedin.com/jobs/search/?geoId=92000000&f_CR=103644278&f_F=it%2Ceng&f_C=' + company_id + '&start=' + str(page * 25)\n",
    "        browser.get(linkedin_jobs_page_url)\n",
    "        \n",
    "        try:\n",
    "            element = WebDriverWait(browser, 3).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'artdeco-pagination__pages--number'))\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            print(\"The page has no pagination\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        time.sleep(random())\n",
    "        \n",
    "        #get a list of jobs from the page\n",
    "        li_list = soup.find_all('li',attrs={'data-occludable-entity-urn' :True}, recursive=True)\n",
    "\n",
    "        for li in li_list:\n",
    "            \n",
    "            #get the id of the job\n",
    "            job_number = re.findall(r\"(?<=_jobPosting:)\\d+?$\", li.attrs['data-occludable-entity-urn'])[0]\n",
    "                    \n",
    "            jobs_computed += 1\n",
    "            print('jobs computed: '' + str(jobs_computed))\n",
    "            \n",
    "            #click on a job to show its details\n",
    "            selector = 'li[data-occludable-entity-urn=\"urn:li:fs_normalized_jobPosting:' + str(job_number) + '\"]'\n",
    "            browser.find_element_by_css_selector(selector).click()\n",
    "            time.sleep(random())\n",
    "\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 2).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"jobs-apply-button--top-card\"))\n",
    "                )\n",
    "            except:\n",
    "                print('Failed to load Button')\n",
    "            \n",
    "            soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "            \n",
    "            #start getting all the information\n",
    "            try:\n",
    "                job_description = soup.find(attrs={\"class\": \"jobs-description-content__text\"}).getText(separator=' ')\n",
    "                job_description = re.sub(r'\\ +', ' ', job_description.replace('\\n', '')).strip()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_description = soup.find(attrs={\"class\": \"jobs-description-content__text\"}).getText(separator=' ')\n",
    "                    job_description = re.sub(r'\\ +', ' ', job_description.replace('\\n', '')).strip()\n",
    "                except:\n",
    "                    job_description = 'not found'\n",
    "                \n",
    "             \n",
    "            try:\n",
    "                applicants = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('\\d\\ applicant')).replace('\\n', '').strip()\n",
    "                applicants = re.sub(r'[a-zA-Z\\ ]', '', applicants)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    applicants = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('\\d\\ applicant')).replace('\\n', '').strip()\n",
    "                    applicants = re.sub(r'[a-zA-Z\\ ]', '', applicants)\n",
    "                except:\n",
    "                    applicants = 'not found'\n",
    "                    \n",
    "            \n",
    "            try:\n",
    "                employees = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('[\\+\\d]\\ employees')).replace('\\n', '').strip()\n",
    "                employees = re.sub(r'[a-zA-Z\\ ]', '', employees)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    employees = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('[\\+\\d]\\ employees')).replace('\\n', '').strip()\n",
    "                    employees = re.sub(r'[a-zA-Z\\ ]', '', employees)\n",
    "                except:\n",
    "                    employees = 'not found'\n",
    "                    \n",
    "            \n",
    "            try:\n",
    "                views = soup.find(attrs={'class': 'jobs-details-top-card__content-container'}).find(text=re.compile('\\d\\ view')).replace('\\n', '').strip()\n",
    "                views = re.sub(r'[a-zA-Z\\ ]', '', views)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    views = soup.find(attrs={'class': 'jobs-details-top-card__content-container'}).find(text=re.compile('\\d\\ view')).replace('\\n', '').strip()\n",
    "                    views = re.sub(r'[a-zA-Z\\ ]', '', views)\n",
    "                except:\n",
    "                    views = 'not found'\n",
    "            \n",
    "            try:\n",
    "                job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                job_date = job_li.find(\"time\")[\"datetime\"]\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                    job_date = job_li.find(\"time\")[\"datetime\"]\n",
    "                except:\n",
    "                    job_date = 'not found'\n",
    "            \n",
    "            try:\n",
    "                job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                location = job_li.find(\"li\", {'class': 'job-card-container__metadata-item'}).get_text()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                    location = job_li.find(\"li\", {'class': 'job-card-container__metadata-item'}).get_text()\n",
    "                except:\n",
    "                    location = 'not found'\n",
    "            \n",
    "            try:\n",
    "                industries_functions = soup.find_all('ul', {\"class\": \"jobs-box__list jobs-description-details__list\"},recursive=True)\n",
    "\n",
    "                title = soup.find('h2', {\"class\": \"jobs-details-top-card__job-title\"},recursive=True).text\n",
    "                title = title.replace('\\n', '').strip()\n",
    "\n",
    "                company = soup.find('a', {\"class\": \"jobs-details-top-card__company-url\"},recursive=True).text\n",
    "                company = company.replace('\\n', '').strip()\n",
    "\n",
    "                industry = []\n",
    "                functions = []\n",
    "                for li in industries_functions[0].find_all('li'):\n",
    "                    industry.append(li.text.replace('\\n', '').strip())\n",
    "                for li in industries_functions[1].find_all('li'):\n",
    "                    functions.append(li.text.replace('\\n', '').strip())\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    industries_functions = soup.find_all('ul', {\"class\": \"jobs-box__list jobs-description-details__list\"},recursive=True)\n",
    "\n",
    "                    title = soup.find('h2', {\"class\": \"jobs-details-top-card__job-title\"},recursive=True).text\n",
    "                    title = title.replace('\\n', '').strip()\n",
    "\n",
    "                    company = soup.find('a', {\"class\": \"jobs-details-top-card__company-url\"},recursive=True).text\n",
    "                    company = company.replace('\\n', '').strip()\n",
    "\n",
    "                    industry = []\n",
    "                    functions = []\n",
    "                    for li in industries_functions[0].find_all('li'):\n",
    "                        industry.append(li.text.replace('\\n', '').strip())\n",
    "                    for li in industries_functions[1].find_all('li'):\n",
    "                        functions.append(li.text.replace('\\n', '').strip())\n",
    "                except:\n",
    "                    industry = ['not found']\n",
    "                    functions = ['not found']\n",
    "            \n",
    "            #grouping job data found\n",
    "            data={\n",
    "                'title': title, \n",
    "                'company': company,\n",
    "                'client id': company_id,\n",
    "                'date': job_date,\n",
    "                'applicants': applicants,\n",
    "                'description': job_description,\n",
    "                'views': views,\n",
    "                'location': location, \n",
    "                'industry': ', '.join(industry), \n",
    "                'job functions': ', '.join(functions)\n",
    "            }\n",
    "            jobs.append(data)\n",
    "            \n",
    "            print(\"titulo: \" + title)\n",
    "            print(\"empresa: \" + company)\n",
    "            print(\"data \" + job_date)\n",
    "            print(\"applicants \" + applicants)\n",
    "            print(\"views \" + views)\n",
    "            print(\"location \" + location)\n",
    "            print(\"description \" + job_description[:40] + '...')\n",
    "            print(\"setores: \" + ', '.join(industry))\n",
    "            print(\"funcoes: \" + ', '.join(functions))\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            \n",
    "            if jobs_computed >= limit:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    #storing company data found\n",
    "    df_info = df_info.append({\n",
    "        'client': company, \n",
    "        'found': True, \n",
    "        'client id': company_id,\n",
    "        'competitors': competitors, \n",
    "        'employees': employees,\n",
    "        'total jobs': total_jobs, \n",
    "        'collected jobs': total_jobs if total_jobs < limit else limit\n",
    "    }, ignore_index=True)\n",
    "    df_info.to_csv(\"client_info.csv\", index=False)\n",
    "    \n",
    "    #storing job data found\n",
    "    try:\n",
    "        df = pd.read_csv(jobs_path)\n",
    "        df_new = pd.DataFrame(jobs)\n",
    "        df = pd.concat([df, df_new])#.drop_duplicates()\n",
    "    except:\n",
    "        df = pd.DataFrame(jobs)\n",
    "    df.to_csv(jobs_path, index=False)\n",
    "    \n",
    "    print(\"**************finished \" + company + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the pseudorandom number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth_linkedin()\n",
    "\n",
    "companies = get_usa_prospects_list()\n",
    "#companies_jobs = get_jobs_companies_list()\n",
    "#to_search = [c for c in companies if c not in companies_jobs]\n",
    "\n",
    "start_index = 0\n",
    "\n",
    "while start_index < len(companies):\n",
    "    \n",
    "    for i in range(start_index, len(companies)):\n",
    "        try:\n",
    "            print(\"Buscando:\", companies[i])\n",
    "            #get_all_jobs(companies[i], \"teste_03_02\", 200)\n",
    "        except:\n",
    "            print(\"Errou em:\", i)\n",
    "            pass\n",
    "        start_index = start_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jobs = pd.read_csv(\"../notebooks-outputs/jobs.csv\")\n",
    "df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_info = pd.read_csv(\"../notebooks-outputs/client_info.csv\")\n",
    "df_client_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_client_info['found'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_client_info.loc[df_client_info['found'] == True]['total jobs'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-im",
   "language": "python",
   "name": "env-im"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
