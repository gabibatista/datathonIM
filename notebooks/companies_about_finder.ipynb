{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e787dcdfd5c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser_setup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBrowserSetup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "import os, random, sys, time, shutil, re, csv, datetime, pickle, math \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.webdriver import Options\n",
    "\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "from ..src.browser.\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_options = Options()\n",
    "# #chrome_options.add_argument('--headless')\n",
    "# chrome_options.add_argument('--no-sandbox')\n",
    "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "# chrome_options.add_argument(\"--log-level=3\")  # fatal\n",
    "# chrome_options.add_argument(\"--start-maximized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = BrowserSetup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(\"https://www.linkedin.com/uas/login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     f = open(\"../cookies/cookies.pkl\")\n",
    "#     f.close()\n",
    "\n",
    "#     print('[Status] Cookies found! Logging in')\n",
    "\n",
    "#     cookies = pickle.load(open(\"../cookies/cookies.pkl\", \"rb\"))\n",
    "\n",
    "#     for cookie in cookies:\n",
    "#         browser.add_cookie(cookie)\n",
    "# except IOError:\n",
    "#     print('[Status] Cookies not found!')\n",
    "\n",
    "#     print('[Status] Reading username and password file')\n",
    "\n",
    "#     config = open('../config/config.txt')\n",
    "#     lines = config.readlines()\n",
    "#     config.close()\n",
    "#     username = lines[0]\n",
    "#     password = lines[1]\n",
    "\n",
    "#     print('[Status] Entering username and password in their respective fields')\n",
    "\n",
    "#     username_field = browser.find_element_by_id('username')\n",
    "#     username_field.send_keys(username)\n",
    "\n",
    "#     password_field = browser.find_element_by_id('password')\n",
    "#     password_field.send_keys(password)\n",
    "\n",
    "#     print('[Status] Submiting username and password...')\n",
    "\n",
    "#     try:\n",
    "#         password_field.submit()\n",
    "\n",
    "#         print('[Status] Saving cookies...')\n",
    "        \n",
    "#         pickle.dump(browser.get_cookies(), open(\"../cookies/cookies.pkl\",\"wb\"))\n",
    "        \n",
    "#         print('[Status] Successfully saved cookies!')\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(browser.get_cookies(), open(\"../cookies/cookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser.current_url\n",
    "# soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "# company_details = soup.find('dl').find_all()\n",
    "# company_details_dict = {}\n",
    "# for num, info in enumerate(company_details):\n",
    "#     if(info.name == 'dt'):\n",
    "#         company_details_dict[info.text.replace('\\n', '').strip()] = company_details[num+1].text.replace('\\n', '').strip()\n",
    "# company_details_dict\n",
    "#print(company_details_dict.get('Website', 'test'))\n",
    "#print(company_details_dict.get('Websites', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_employees_number(companies):\n",
    "    \n",
    "    #opening the csv to save the informations about the client\n",
    "    try:\n",
    "        df_info = pd.read_csv('companies_size.csv')\n",
    "    except:\n",
    "        df_info = pd.DataFrame()\n",
    "    \n",
    "    for processed, company in enumerate(companies):\n",
    "        print(\"processed: \" + str(processed))\n",
    "        #searching for the company's linkedin page\n",
    "        print(\"searching for the company \" + company)\n",
    "        #print(browser.current_url)\n",
    "        browser.get(\"https://www.linkedin.com/search/results/companies/?keywords=\" + company)\n",
    "        time.sleep(2 + random())\n",
    "        \n",
    "        #getting company id\n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        code = soup.find_all('code')\n",
    "\n",
    "        company_id = ''\n",
    "\n",
    "        try:\n",
    "            for code_str in code:\n",
    "                if('urn:li:fsd_company:' in code_str.text):\n",
    "                    company_id = re.findall(r\"(?<=fsd_company:)\\d+?(?=[,|\\\"])\", code_str.text)[0]\n",
    "                    #print(company_id)\n",
    "                    if(company_id != '8175'):\n",
    "                        break\n",
    "            if company_id == '' or company_id == '8175':\n",
    "                raise Exception(\"No Id for \" + company)\n",
    "        except:\n",
    "\n",
    "            print(\"company \" + company + \" not found\\n\")\n",
    "            print(browser.current_url)\n",
    "            if \"results/companies\" not in browser.current_url: \n",
    "                print('logged out...')\n",
    "                return\n",
    "            info_to_append = {\n",
    "                'company_searched': company, \n",
    "                'company_found': '', \n",
    "                'found': False, \n",
    "                'company_id': '', \n",
    "                'employees': '',\n",
    "                'industry': '',\n",
    "                'specialties': '',\n",
    "                'headquarters': ''\n",
    "                \n",
    "            }\n",
    "            df_info = df_info.append(info_to_append, ignore_index=True)\n",
    "            df_info.to_csv(\"companies_size.csv\", index=False)\n",
    "            print('---------------------------------------------')\n",
    "            continue \n",
    "\n",
    "        #accessing the company's page\n",
    "        linkedin_url = \"https://www.linkedin.com/company/\" + company_id + \"/about/\"\n",
    "        browser.get(\"https://www.linkedin.com/company/\" + company_id + \"/about/\")\n",
    "        time.sleep(2 + random())\n",
    "        \n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        \n",
    "        #searching for the company title\n",
    "        try:\n",
    "            company_title = soup.find(attrs={\"class\": \"org-top-card-summary__title\"})\n",
    "            company_title = company_title.find(\"span\").getText()\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                company_title = soup.find(attrs={\"class\": \"org-top-card-summary__title\"})\n",
    "                company_title = company_title.find(\"span\").getText()\n",
    "            except:\n",
    "                company_title = ''\n",
    "        \n",
    "        #searching for the number of employees\n",
    "        try:\n",
    "            employees = soup.find(attrs={\"class\": \"org-about-company-module__company-size-definition-text\"}).getText().replace('\\n', '').strip()\n",
    "            print(\"employees:\", employees)\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                employees = soup.find(attrs={\"class\": \"org-about-company-module__company-size-definition-text\"}).getText().replace('\\n', '').strip()\n",
    "            except:\n",
    "                employees = 'not found'\n",
    "        \n",
    "        #searching for the company info:\n",
    "        company_details_dict = {}\n",
    "        try:\n",
    "            company_details = soup.find('dl').find_all()\n",
    "            for num, info in enumerate(company_details):\n",
    "                if(info.name == 'dt'):\n",
    "                    company_details_dict[info.text.replace('\\n', '').strip()] = company_details[num+1].text.replace('\\n', '').strip()\n",
    "            print(\"company details:\", company_details_dict)\n",
    "        except:\n",
    "            print(\"Pass\")\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        browser.get('https://www.linkedin.com/jobs/search/?geoId=92000000&f_CR=103644278&f_F=it%2Ceng&f_C=' + company_id)\n",
    "        time.sleep(1 + random())\n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        \n",
    "        iteng_jobs = ''\n",
    "        try:\n",
    "            iteng_jobs = re.search(r\"(.+\\ )\", soup.find(\"div\", attrs={\"class\": \"jobs-search-results-list__title-heading\"}).find(\"small\").text).group(1).strip()\n",
    "        except:\n",
    "            iteng_jobs = 'not found'\n",
    "        \n",
    "        \n",
    "        #storing company data found\n",
    "        print('-----------DADOS-----------')\n",
    "        print('company_searched', company)\n",
    "        print('company_found', company_title) \n",
    "        print('company_id', company_id)\n",
    "        print('linkedin', linkedin_url)\n",
    "        print('iteng_jobs', iteng_jobs)\n",
    "        print('website', company_details_dict.get('Site', ''))\n",
    "        print('employees', company_details_dict.get('Tamanho da empresa', ''))\n",
    "        print('industry', company_details_dict.get('Setor', ''))\n",
    "        print('specialties', company_details_dict.get('Especializações', ''))\n",
    "        print('headquarters', company_details_dict.get('Sede', ''))\n",
    "        \n",
    "        info_to_append = {\n",
    "            'company_searched': company,\n",
    "            'company_found': company_title, \n",
    "            'found': True, \n",
    "            'company_id': company_id,\n",
    "            'linkedin': linkedin_url,\n",
    "            'it_eng_jobs': iteng_jobs,\n",
    "            'website': company_details_dict.get('Site', ''),\n",
    "            'employees': company_details_dict.get('Tamanho da empresa', ''),\n",
    "            'industry': company_details_dict.get('Setor', ''),\n",
    "            'specialties': company_details_dict.get('Especializações', ''),\n",
    "            'headquarters': company_details_dict.get('Sede', '')\n",
    "        }\n",
    "        \n",
    "        df_info = df_info.append(info_to_append, ignore_index=True)\n",
    "        df_info.to_csv(\"companies_size.csv\", index=False)\n",
    "        print('---------------------------------------------')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_searched():\n",
    "    try:\n",
    "        searched = pd.read_csv('companies_size.csv')['company_searched'].to_list()\n",
    "        return list(filter(lambda elm: isinstance(elm, str), searched))\n",
    "    except:\n",
    "        return []\n",
    "len(get_company_searched())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_list():\n",
    "    try:\n",
    "        companies = pd.read_csv('Companies.csv')['Company'].to_list()\n",
    "        return list(filter(lambda elm: isinstance(elm, str), companies))\n",
    "    except:\n",
    "        return []\n",
    "len(get_company_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "companies = get_company_list()\n",
    "companies_searched = get_company_searched()\n",
    "to_search = [c for c in companies if c not in companies_searched]\n",
    "\n",
    "while len(to_search) > 100:\n",
    "    \n",
    "    companies = get_company_list()\n",
    "    companies_searched = get_company_searched()\n",
    "    to_search = [c for c in companies if c not in companies_searched]\n",
    "    \n",
    "    count = int(20 + 50*random())\n",
    "    print(count)\n",
    "    \n",
    "    print(\"Buscando total de:\", len(to_search))\n",
    "    get_employees_number(to_search[0:count])\n",
    "    \n",
    "    timeSleep = 30 + 60*random()\n",
    "    print(\"Esperando um total de:\", timeSleep)\n",
    "    \n",
    "    time.sleep(timeSleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('companies_size.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('companies_size.csv')['found'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python datathon-dna",
   "language": "python",
   "name": "datathon-dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
