{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time, shutil, re, csv, datetime, pickle, math \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.webdriver import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "#chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(\"--log-level=3\")  # fatal\n",
    "chrome_options.add_argument(\"--start-maximized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get(\"https://www.linkedin.com/uas/login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open(\"../cookies/cookies.pkl\")\n",
    "    f.close()\n",
    "\n",
    "    print('[Status] Cookies found! Logging in')\n",
    "\n",
    "    cookies = pickle.load(open(\"../cookies/cookies.pkl\", \"rb\"))\n",
    "\n",
    "    for cookie in cookies:\n",
    "        browser.add_cookie(cookie)\n",
    "except IOError:\n",
    "    print('[Status] Cookies not found!')\n",
    "\n",
    "    print('[Status] Reading username and password file')\n",
    "\n",
    "    config = open('../config/config.txt')\n",
    "    lines = config.readlines()\n",
    "    config.close()\n",
    "    username = lines[0]\n",
    "    password = lines[1]\n",
    "\n",
    "    print('[Status] Entering username and password in their respective fields')\n",
    "\n",
    "    username_field = browser.find_element_by_id('username')\n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    password_field = browser.find_element_by_id('password')\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    print('[Status] Submiting username and password...')\n",
    "\n",
    "    try:\n",
    "        password_field.submit()\n",
    "\n",
    "        print('[Status] Saving cookies...')\n",
    "        \n",
    "        pickle.dump(browser.get_cookies(), open(\"../cookies/cookies.pkl\",\"wb\"))\n",
    "        \n",
    "        print('[Status] Successfully saved cookies!')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(browser.get_cookies(), open(\"../cookies/cookies.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the pseudorandom number generator\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs(company, competitors, limit):\n",
    "    \n",
    "    #opening the csv to save the informations about the client\n",
    "    try:\n",
    "        df_info = pd.read_csv('client_info.csv')\n",
    "    except:\n",
    "        df_info = pd.DataFrame()\n",
    "    \n",
    "    #searching for the company's linkedin page\n",
    "    print(\"searching for the company \" + company)\n",
    "    browser.get(\"https://www.linkedin.com/search/results/companies/?keywords=\" + company)\n",
    "    time.sleep(1 + random())\n",
    "    \n",
    "    #getting company id\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    code = soup.find_all('code')\n",
    "    \n",
    "    company_id = ''\n",
    "    \n",
    "    try:\n",
    "        for code_str in code:\n",
    "            if('urn:li:fsd_company:' in code_str.text):\n",
    "                company_id = re.findall(r\"(?<=fsd_company:)\\d+?(?=[,|\\\"])\", code_str.text)[0]\n",
    "                print(company_id)\n",
    "                if(company_id != '8175'):\n",
    "                    break\n",
    "        if company_id == '':\n",
    "            raise Exception(\"No company Id\")\n",
    "    except:\n",
    "        print(\"company \" + company + \" not found\\n\")\n",
    "        df_info = df_info.append({\n",
    "            'client': company, \n",
    "            'found': False, \n",
    "            'client id': '',\n",
    "            'competitors': competitors,\n",
    "            'employees': '',\n",
    "            'total jobs': '', \n",
    "            'collected jobs': ''\n",
    "        }, ignore_index=True)\n",
    "        df_info.to_csv(\"client_info.csv\", index=False)\n",
    "        return \n",
    "        \n",
    "    #accessing the company's job offers\n",
    "    browser.get(\"https://www.linkedin.com/jobs/search/?geoId=92000000&f_C=\" + company_id )\n",
    "    \n",
    "    #getting the total pages of the job offers\n",
    "    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "    \n",
    "    try:\n",
    "        total_jobs = re.search(r\"(.+\\ )\", soup.find(\"div\", attrs={\"class\": \"jobs-search-results-list__title-heading\"}).find(\"small\").text).group(1).strip()\n",
    "    except:\n",
    "        print(\"no jobs found for \" + company + '\\n')\n",
    "        df_info = df_info.append({\n",
    "            'client': company, \n",
    "            'found': True,\n",
    "            'client id': str(company_id),\n",
    "            'competitors': competitors, \n",
    "            'employees': '',\n",
    "            'total jobs': '0', \n",
    "            'collected jobs': '0'\n",
    "        }, ignore_index=True)\n",
    "        df_info.to_csv(\"client_info.csv\", index=False)\n",
    "        return\n",
    "    \n",
    "    total_jobs = int(re.sub(r'[\\.|,]', '', total_jobs))\n",
    "    \n",
    "    pages = math.ceil(int(total_jobs) / 25)\n",
    "    print(str(total_jobs) + ' jobs and ' + str(pages) + ' pages found for ' + company + \"\\n\")\n",
    "\n",
    "    #getting the information from all the jobs\n",
    "    jobs_computed = 0\n",
    "    jobs = []\n",
    "    \n",
    "    for page in range(pages):\n",
    "        \n",
    "        #access a job page\n",
    "        browser.get('https://www.linkedin.com/jobs/search/?geoId=92000000&f_CR=103644278&f_F=it%2Ceng&f_C=' + company_id + '&start=' + str(page * 25))\n",
    "        \n",
    "        try:\n",
    "            element = WebDriverWait(browser, 3).until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CLASS_NAME, \"artdeco-pagination__pages--number\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        except:\n",
    "            print(\"The page has no pagination\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "        time.sleep(random())\n",
    "        \n",
    "        #get a list of jobs from the page\n",
    "        li_list = soup.find_all(\"li\",attrs={\"data-occludable-entity-urn\" :True}, recursive=True)\n",
    "\n",
    "        for li in li_list:\n",
    "            \n",
    "            #get the id of the job\n",
    "            job_number = re.findall(r\"(?<=_jobPosting:)\\d+?$\", li.attrs['data-occludable-entity-urn'])[0]\n",
    "                    \n",
    "            jobs_computed += 1\n",
    "            print(\"jobs computed: \" + str(jobs_computed))\n",
    "            \n",
    "            #click on a job to show its details\n",
    "            selector = 'li[data-occludable-entity-urn=\"urn:li:fs_normalized_jobPosting:' + str(job_number) + '\"]'\n",
    "            browser.find_element_by_css_selector(selector).click()\n",
    "            time.sleep(random())\n",
    "\n",
    "            try:\n",
    "                element = WebDriverWait(browser, 2).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"jobs-apply-button--top-card\"))\n",
    "                )\n",
    "            except:\n",
    "                print(\"Failed to load Button\")\n",
    "            \n",
    "            soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "            \n",
    "            #start getting all the information\n",
    "            try:\n",
    "                job_description = soup.find(attrs={\"class\": \"jobs-description-content__text\"}).getText(separator=' ')\n",
    "                job_description = re.sub(r'\\ +', ' ', job_description.replace('\\n', '')).strip()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_description = soup.find(attrs={\"class\": \"jobs-description-content__text\"}).getText(separator=' ')\n",
    "                    job_description = re.sub(r'\\ +', ' ', job_description.replace('\\n', '')).strip()\n",
    "                except:\n",
    "                    job_description = 'not found'\n",
    "                \n",
    "             \n",
    "            try:\n",
    "                applicants = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('\\d\\ applicant')).replace('\\n', '').strip()\n",
    "                applicants = re.sub(r'[a-zA-Z\\ ]', '', applicants)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    applicants = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('\\d\\ applicant')).replace('\\n', '').strip()\n",
    "                    applicants = re.sub(r'[a-zA-Z\\ ]', '', applicants)\n",
    "                except:\n",
    "                    applicants = 'not found'\n",
    "                    \n",
    "            \n",
    "            try:\n",
    "                employees = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('[\\+\\d]\\ employees')).replace('\\n', '').strip()\n",
    "                employees = re.sub(r'[a-zA-Z\\ ]', '', employees)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    employees = soup.find(attrs={'class': 'jobs-details-job-summary'}).find(text=re.compile('[\\+\\d]\\ employees')).replace('\\n', '').strip()\n",
    "                    employees = re.sub(r'[a-zA-Z\\ ]', '', employees)\n",
    "                except:\n",
    "                    employees = 'not found'\n",
    "                    \n",
    "            \n",
    "            try:\n",
    "                views = soup.find(attrs={'class': 'jobs-details-top-card__content-container'}).find(text=re.compile('\\d\\ view')).replace('\\n', '').strip()\n",
    "                views = re.sub(r'[a-zA-Z\\ ]', '', views)\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    views = soup.find(attrs={'class': 'jobs-details-top-card__content-container'}).find(text=re.compile('\\d\\ view')).replace('\\n', '').strip()\n",
    "                    views = re.sub(r'[a-zA-Z\\ ]', '', views)\n",
    "                except:\n",
    "                    views = 'not found'\n",
    "            \n",
    "            try:\n",
    "                job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                job_date = job_li.find(\"time\")[\"datetime\"]\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                    job_date = job_li.find(\"time\")[\"datetime\"]\n",
    "                except:\n",
    "                    job_date = 'not found'\n",
    "            \n",
    "            try:\n",
    "                job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                location = job_li.find(\"li\", {'class': 'job-card-container__metadata-item'}).get_text()\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    job_li = soup.find('li',{'data-occludable-entity-urn': 'urn:li:fs_normalized_jobPosting:' + job_number})\n",
    "                    location = job_li.find(\"li\", {'class': 'job-card-container__metadata-item'}).get_text()\n",
    "                except:\n",
    "                    location = 'not found'\n",
    "            \n",
    "            try:\n",
    "                industries_functions = soup.find_all('ul', {\"class\": \"jobs-box__list jobs-description-details__list\"},recursive=True)\n",
    "\n",
    "                title = soup.find('h2', {\"class\": \"jobs-details-top-card__job-title\"},recursive=True).text\n",
    "                title = title.replace('\\n', '').strip()\n",
    "\n",
    "                company = soup.find('a', {\"class\": \"jobs-details-top-card__company-url\"},recursive=True).text\n",
    "                company = company.replace('\\n', '').strip()\n",
    "\n",
    "                industry = []\n",
    "                functions = []\n",
    "                for li in industries_functions[0].find_all('li'):\n",
    "                    industry.append(li.text.replace('\\n', '').strip())\n",
    "                for li in industries_functions[1].find_all('li'):\n",
    "                    functions.append(li.text.replace('\\n', '').strip())\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    soup = BeautifulSoup(browser.page_source, features='html.parser')\n",
    "                    industries_functions = soup.find_all('ul', {\"class\": \"jobs-box__list jobs-description-details__list\"},recursive=True)\n",
    "\n",
    "                    title = soup.find('h2', {\"class\": \"jobs-details-top-card__job-title\"},recursive=True).text\n",
    "                    title = title.replace('\\n', '').strip()\n",
    "\n",
    "                    company = soup.find('a', {\"class\": \"jobs-details-top-card__company-url\"},recursive=True).text\n",
    "                    company = company.replace('\\n', '').strip()\n",
    "\n",
    "                    industry = []\n",
    "                    functions = []\n",
    "                    for li in industries_functions[0].find_all('li'):\n",
    "                        industry.append(li.text.replace('\\n', '').strip())\n",
    "                    for li in industries_functions[1].find_all('li'):\n",
    "                        functions.append(li.text.replace('\\n', '').strip())\n",
    "                except:\n",
    "                    industry = ['not found']\n",
    "                    functions = ['not found']\n",
    "            \n",
    "            #grouping job data found\n",
    "            data={\n",
    "                \"title\": title, \n",
    "                \"company\": company,\n",
    "                \"client id\": company_id,\n",
    "                \"date\": job_date,\n",
    "                \"applicants\": applicants,\n",
    "                \"description\": job_description,\n",
    "                \"views\": views,\n",
    "                \"location\": location, \n",
    "                \"industry\": ', '.join(industry), \n",
    "                \"job functions\": ', '.join(functions)\n",
    "            }\n",
    "            jobs.append(data)\n",
    "            \n",
    "            print(\"titulo: \" + title)\n",
    "            print(\"empresa: \" + company)\n",
    "            print(\"data \" + job_date)\n",
    "            print(\"applicants \" + applicants)\n",
    "            print(\"views \" + views)\n",
    "            print(\"location \" + location)\n",
    "            print(\"description \" + job_description[:40] + '...')\n",
    "            print(\"setores: \" + ', '.join(industry))\n",
    "            print(\"funcoes: \" + ', '.join(functions))\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            \n",
    "            if jobs_computed >= limit:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    #storing company data found\n",
    "    df_info = df_info.append({\n",
    "        'client': company, \n",
    "        'found': True, \n",
    "        'client id': company_id,\n",
    "        'competitors': competitors, \n",
    "        'employees': employees,\n",
    "        'total jobs': total_jobs, \n",
    "        'collected jobs': total_jobs if total_jobs < limit else limit\n",
    "    }, ignore_index=True)\n",
    "    df_info.to_csv(\"client_info.csv\", index=False)\n",
    "    \n",
    "    #storing job data found\n",
    "    try:\n",
    "        df = pd.read_csv('jobs.csv')\n",
    "        df_new = pd.DataFrame(jobs)\n",
    "        df = pd.concat([df, df_new])#.drop_duplicates()\n",
    "    except:\n",
    "        df = pd.DataFrame(jobs)\n",
    "    df.to_csv(\"jobs.csv\", index=False)\n",
    "    \n",
    "    print(\"**************finished \" + company + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usa_prospects_list():\n",
    "    return pd.read_csv('usa_selection.csv')['prospects'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_companies_list():\n",
    "    return pd.read_csv('jobs.csv')['company'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "companies = get_usa_prospects_list()\n",
    "#companies_jobs = get_jobs_companies_list()\n",
    "#to_search = [c for c in companies if c not in companies_jobs]\n",
    "\n",
    "start_index = 39\n",
    "\n",
    "while start_index < len(companies):\n",
    "    \n",
    "    for i in range(start_index, len(companies)):\n",
    "        try:\n",
    "            print(\"Buscando:\", companies[i])\n",
    "            #get_all_jobs(companies[i], \"teste_03_02\", 200)\n",
    "        except:\n",
    "            print(\"Errou em:\", i)\n",
    "            pass\n",
    "        start_index = start_index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jobs = pd.read_csv(\"jobs.csv\")\n",
    "df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client_info = pd.read_csv(\"client_info.csv\")\n",
    "df_client_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_client_info['found'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_client_info.loc[df_client_info['found'] == True]['total jobs'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
